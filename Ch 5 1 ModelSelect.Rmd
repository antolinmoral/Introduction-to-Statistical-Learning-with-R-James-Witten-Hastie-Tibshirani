---
title: "Ch 5 1 ModelSelect"
output: html_document
---

Analyze the ISLR (Introduction to Statistical Learning with R) data package's baseball 'Hitters' data frame:

```{r}
library(ISLR)
summary(Hitters)
```

There are missing values, before we proceed we will remove them:

```{r}
with(Hitters, sum(is.na(Salary)))
Hitters=na.omit(Hitters)
with(Hitters, sum(is.na(Salary)))
```

Best Subset regression
----------------------
We will now use the package `leaps` to evaluate all the best-subset models.
```{r}
library(leaps)
regfit.full = regsubsets(Salary~., data=Hitters)
summary(regfit.full)
```

By default, it gives the first 8 variables best-subset models. Let's do it again for all
the variables:

```{r}
regfit.full = regsubsets(Salary~., data=Hitters, nvmax=19)
reg.summary = summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp, xlab="Number of variables", ylab="Cp")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], pch=20, col="red")
```

There is a method for the `regsubset` object:

```{r}
plot(regfit.full,scale="Cp")
coef(regfit.full, 10)
```

Forward Stepwise Selection
--------------------------

We use `regsubset` again, but specify the `method="forward"` option.

```{r}
regfit.fwd = regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
plot(regfit.fwd, scale="Cp")
```

Model Selection Using a Validation Set
--------------------------------------

Let's make a training and validation set, so that we can choose a good subset model.

```{r}
dim(Hitters)
set.seed(1)
train = sample(seq(263),180,replace=FALSE)
regfit.fwd = regsubsets(Salary~., data=Hitters[train,], nvmax=19, method="forward")
```

Now, we separate the data to two parts, one for training and another for test/validation to make prediction. There is no `prediction` method for `regsubsets`, so we need to write that part. We also create a vector to store the results of the 19 different models.

```{r}
val.errors = rep(NA, 19)
x.test = model.matrix(Salary~., data=Hitters[-train,])
for(i in 1:19){
  coefi = coef(regfit.fwd, id=i)
  pred = x.test[,names(coefi)]%*%coefi
  val.errors[i] = mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors), ylab="Root MSE", ylim=c(300,400), pch=19, type="b")
points(sqrt(regfit.fwd$rss[-1]/180),col="blue",pch=19,type="b") # -1 excludes null model
legend("topright", legend=c("Training","Validation"),col=c("blue","black"),pch=19)
```

As expected, the model error goes down monotonically as the model gets bigger, but not so for the validation error.

This was a little tedious - not having a `predict` method for `regsubsets`. So we will write one!

```{r}
predict.regsubsets = function(object, newdata, id, ...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id=id)
  mat[, names(coefi)] %*% coefi
}
as.formula(regfit.fwd$call[[2]])
```

Model Seletion by Cross-Validation
----------------------------------

We will do a 10-fold cross-validation.

```{r}
set.seed(11)
folds = sample(rep(1:10,length=nrow(Hitters)))
table(folds)
cv.errors = matrix(NA,10,19)
for(k in 1:10){ # Loop for folds
  best.fit = regsubsets(Salary~., data=Hitters[folds!=k,],nvmax=19,method="forward")
  for(i in 1:19){ # Loop for sizes of picked best subsets of features 
    pred = predict(best.fit,Hitters[folds==k,], id=i)
    cv.errors[k,i] = mean((Hitters$Salary[folds==k]-pred)^2)
  }
}
rmse.cv = sqrt(apply(cv.errors,2,mean))
plot(rmse.cv, pch=19, type="b")
```

Ridge regression and the Lasso
------------------------------

We will use the `glmnet` package, which does not use the formula language, so we will set up an `x` and `y`.

```{r}
library(glmnet)
x=model.matrix(Salary~.-1, data=Hitters) # -1 means no intercept
y=Hitters$Salary
```

First, we will fit a ridge regression model. This is achived by calling `glmnet` with `alpha=0` (see the help file). There is also a `cv.glmnet` function, which will do the cross validation for us.

```{r}
fit.ridge = glmnet(x,y,alpha=0)
plot(fit.ridge, xvar="lambda", label=TRUE)
cv.ridge = cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```

Now, we fit the lasso; in `glmnet` it means `alpha=1`

```{r}
fit.lasso = glmnet(x,y)
plot(fit.lasso, xvar="lambda", label=TRUE)
plot(fit.lasso, xvar="dev", label=TRUE)
cv.lasso = cv.glmnet(x,y)
plot(cv.lasso)
coef(cv.lasso) # Picks model one std from minimum to make more parsimonius  
```

Suppose we want our earlier train/validation sets to select the `lambda` for the lasso.

```{r}
lasso.tr = glmnet(x[train,], y[train])
lasso.tr
pred = predict(lasso.tr, x[-train,])
dim(pred)
rmse = sqrt(apply((y[-train]-pred)^2, 2, mean))
plot(log(lasso.tr$lambda), rmse, type="b", xlab="Log(lambda)")
lam.best = lasso.tr$lambda[order(rmse)[1]] # Pick best lambda
lam.best
coef(lasso.tr, s=lam.best) # sparse matrix format 
```


