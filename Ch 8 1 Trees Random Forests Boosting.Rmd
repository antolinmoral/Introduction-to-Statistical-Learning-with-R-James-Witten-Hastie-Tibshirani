---
title: "Trees-Random-Forest-Boosting"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Decision Tree
=============

We will examine the `Carseats` data using the `tree` package in R, 
as in the lab in the book (Introduction to Statistical Learning with R).

We create a binary response variable `High` (for high sales) and we include
it in the same dataframe.

```{r}
require(ISLR)
require(tree)
attach(Carseats)
View(Carseats)
hist(Sales)
High = ifelse(Sales<=8, "No", "Yes")
Carseats = data.frame(Carseats, High)
```

Now, we fit a tree to these data, and summarize and plot it. Notice we have to _exclude_ `Sales` from the right-hand side of the formula, because the response is derived from it. This fit is the simplest possible,
there are two possible outcomes.

```{r}
tree.carseats = tree(High~.-Sales, data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
```

For a detailed summary of the tree, print it:

```{r}
tree.carseats
```

Let's create training and test sets (250, 150) of the 400 observations. Grow the tree on the training set
and evaluate the tree on the test set.

```{r}
set.seed(1011)
train = sample(1:nrow(Carseats), 250)
tree.carseats = tree(High~.-Sales, Carseats, subset = train)
plot(tree.carseats); text(tree.carseats, pretty = 0)
tree.pred = predict(tree.carseats, Carseats[-train,], type="class")
with(Carseats[-train,], table(tree.pred, High))
cat("Error rate:", (72+33)/150)
```

This tree was grown to full depth, amd might be too variable. We now use CV to prune it.

```{r}
cv.carseats = cv.tree(tree.carseats, FUN=prune.misclass)
cv.carseats
plot(cv.carseats) # 13 nodes seem to be enough to keep
prune.carseats = prune.misclass(tree.carseats, best=13)
plot(prune.carseats); text(prune.carseats, pretty = 0)
```

Now, let's evaluate this pruned tree on the test data:

```{r}
tree.pred = predict(prune.carseats, Carseats[-train,], type="class")
with(Carseats[-train,], table(tree.pred, High))
cat("Confusion table", (72+32)/150)
```

Did not get much from pruning, except for a shallower tree, which is easier to interpret.



Random forest and Boosting
==========================

These models use trees as building blocks to build more complex models. 

Here we will use the Boston housing data to explore random forest and boosting. These data
are in the `MASS` package. It gives housing values and other statistics  in each of the 506 
suburbs of Boston based on a 1970 census.

Random Forests
--------------
Random forests build bushy trees and then average them to reduce variance.

```{r}
require(randomForest)
require(MASS)
set.seed(101)
dim(Boston) # (505, 14)
train = sample(1:nrow(Boston), 300)
?Boston
```


Let's fit a random forest and see how well it performs. We will use
the `medv`, the median housing value (in \$1K dollars)

```{r}
rf.boston = randomForest(medv~., data=Boston, subset=train)
rf.boston
```

The MSR and the % variance explained on OOB  or out-of-bag estimates, 
a very clever device in random forests to get honest error estimates.
The model reports that `mtry=4`, which is the number of variables randomly
chosen at each split. Since $p=13$ here, we could try all 13 possible 
values of `mtry`. We will do so, record the results, and make a plot.

```{r}
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
  fit = randomForest(medv~., data=Boston, subset=train, mtry=mtry, ntree=400)
  oob.err[mtry] = fit$mse[400]
  pred = predict(fit, Boston[-train,])
  test.err[mtry] = with(Boston[-train,], mean((medv-pred)^2))
  cat(mtry, " ")
}
matplot(1:mtry, cbind(test.err, oob.err), pch=19, col=c("red","blue"), type="b", ylab="Mean Squared Error")
legend("topright", legend=c("Test","OOB"), pch=19, col=c("red","blue"))
```

Here mtry=1 corresponds to a single tree (though no prining here as in the simple
decision tree case), while mtry=13 corresponds to bagging, when all 13 variables
are used in every split in the tree.


